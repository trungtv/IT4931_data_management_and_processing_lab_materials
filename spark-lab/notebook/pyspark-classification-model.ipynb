{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Spark is a hot topic on big data analytics. Spark is an analytics engine for big data processing. It provides high-level APIs in Python known as pySpark. Spark also supports Scala which is also its native language, Java and R. Spark code is written in Java for optimizing for the big data and pySpark is the python wrapper to connect the python IDE to the Java engine. As soon as big data and distributed computing become of the mainstream in analytics, Spark is going to be the number one tool in demand in the near future. But the good news is Spark API is very simple to learn. Jupyter notebook not only supports Spark but also recommended running pySpark code for an interactive environment. We will focus pySpark API in Jupyter notebook in this notebook.\n",
    "\n",
    "\n",
    "### Spark SQL \n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. One obvious use of Spark SQL is to execute SQL queries. Unlike the name suggest Spark SQL is not just about SQL. Spark SQL also contains two ther objects: dataset and dataframe. A Dataset is a distributed collection of data. Python does not have the support for the Dataset API of Spark. However, Python supports Spark dataframe which is similar to Pandas in Python. A DataFrame is a Dataset organized into named columns. However, internally it is designed to scale the data analysis for big data. One of the aims of this notebook is to see what Spark dataframe is capable of doing and compare and contrast with the Pandas dataframe.\n",
    "\n",
    "\n",
    "### Spark RDDs\n",
    "\n",
    "In parallel to the Spark data frame there is another object called Spark RDDs. RDDs stands for a resilient distributed dataset, which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. Spark dataframe is a later addition to the Spark and is more versatile and superior to RDDs. Spark dataframe is faster than RDDs and resemble dataframe in Pandas. Spark dataframe is preferred over RDDs for this reason. Also when doing the machine learning in Spark there are two separate libraries in Spark one is for RDDs which is called MLlib. The second machine learning library in Spark is dataframe based and is known as Spark ML. A source of confusion: in Spark documentation both ML libraries are referred to as MLlib. In Spark community they, usually they are referred to as Spark ML and MLlib because they are called with spark.ml and spark.mllib respectively when writing code. Spark documentation also announced DataFrame-based API is the primary API for the future. We will not talk RDDs and MLlib further in this notebook. Rather we will discuss dataframe and Spark ML.\n",
    "\n",
    "\n",
    "### Spark ML \n",
    "\n",
    "The Spark ML library is primary library of Spark for machine learning. Spark ML mimics the API of sci-kit learn for Python user. Internally it is designed to make machine learning scalable for big data. Pretty much similar to sci-kit learn Spark ML has the following features: \n",
    "- machine learning algorithms such as classification, regression, clustering, and collaborative filtering. \n",
    "- Feature extraction, transformation, dimensionality reduction, and selection. \n",
    "- Tools for constructing, evaluating, and tuning machine learning pipelines. \n",
    "- Saving and load algorithms, models, and pipelines. \n",
    "- Linear algebra, statistics, data handling, etc. \n",
    "\n",
    "The Spark ML library is not as big as Sklearn but it is growing surely and steadily. We will use Spark ML API in this notebook to perform machine learning tasks. Especially we study logistic regression with Ridge and Lasso regularization and two popular tree-based ensemble learning methods, random forest and\n",
    "gradient boosting.\n",
    "\n",
    "\n",
    "\n",
    "In addition to those Spark has GraphX library for graph processing, and Spark Streaming for Structured Streaming for incremental computation and stream processing which we will not discuss here.\n",
    "\n",
    "\n",
    "### Transformations & Actions \n",
    "\n",
    "The main purpose of Spark is to make calculations on very large scale datasets presented in distributed format. The data usually exists in a cluster of computers and the situation is very different from analyzing the data which can accommodate in a single computer and can be loaded in memory. But Spark also supports running in a single machine. And this is the best way to start learning Spark. But even doing so, special care should be taken to write the Spark code with big data case in mind so that we do not blast the memory of the local engine when working with the big data. We follow spark best practice when writing code.\n",
    "\n",
    "Generally, there are two types of operations in spark: transformations and actions. Transformations construct a new dataframe from a previous one. Actions, on the other hand, compute a result based on a dataframe, and either return it to the driver program or save it to an external storage system. For example, let's say we have a dataframe. If we select only a few columns from the dataset this is an example of transformation. Doing so we create a new dataframe. But spark do not immediately execute such operation. Remember that the dataset Spark designed to handle is a big one. Executing means using storage resources. Usually, we do a series of transformations doing the analysis. Computing the final result might be much lighter for the memory. For example, if our goal is to calculate the average of each of the selected columns the end result is just a few numbers. In this example calculating average is an action. Action demands some execution on the dataframe. Here is another example of transformation in the context of machine learning: ML model is a transformation which transforms a DataFrame with features into a DataFrame with predictions. On the other hand, a learning algorithm making predictions and calculating the scores to evaluate the quality of the model are  actions which trains on a DataFrame and produces a result.\n",
    "\n",
    "So in this sense, we say Spark evaluates the transformation in a lazy fashion. One of the drawbacks of the lazy evaluation can be understood from the following example. If we select some columns from the dataframe and calculate the mean and it gives us an answer. And then we have to calculate the standard deviation, then Spark again has to go through selecting the same column and then calculate the standard deviation. For big data, this route can be more effective. But if the same transformation is needed for many actions it might be convenient to store the intermediate result in memory. This process is called a cache. We will not discuss this further here. \n",
    "\n",
    "\n",
    "# Starting pySpark\n",
    "\n",
    "Spark is not available in Kaggle notebook by default. But installing pySpark in Kaggle kernel is easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to start a SparkSession and create a spark instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import count, mean, when, lit, create_map, regexp_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV files in Spark is not that different than in Pandas. The new thing here is the schema of the dataframe. Spark schema is the structure of the DataFrame or Dataset. The columns in the dataframe can be integer or float or string. If we enable inferring schema while reading the dataframe it finds the right schema for each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('../input/titanic/train.csv',\\\n",
    "                     header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('../input/titanic/test.csv', \\\n",
    "                     header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the schema here. There is no need to print the column names separately. Although this option is available in Spark. We will use it shortly for a different purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has head(), tail(), sample() options to view few rows of the dataframe. Spark dataframe also has all of those options which you can try. But here we use show(n) method to view sample rows of the dataframe. By default show() presents 20 rows. Please remember the discussion of transformation vs action above. All of these are example of Spark action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the show() might look ugly, especially if there are a large number of columns in the dataframe. At this point, we might miss Pandas head(). There is an option to convert the Spark dataframe into the Pandas dataframe. But we have to be careful here. Usually, Spark is handling a large volume of data, and converting it to Pandas stores everything immediately to the memory. Which we should avoid for the large data. However, there is a way out. Please remember the lazy evaluation of the Spark transformation. We can transform the Spark dataframe by limit(n) to take only n number of rows and then convert that to the Pandas. toPandas() is an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can select() a few columns and inspect within Spark. select() is an example of Spark transformation. Therefore that step is evaluated lazily. Hence we pass a Spark action show() at the end to print the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('Survived', 'Pclass', 'Age', 'Fare').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has describe() method similar to the Pandas. But I find a summary() method more versatile than describe(). Please check [here](https://github.com/roshankoirala/pySpark_tutorial/blob/master/Exploratory_data_analysis_with_pySpark.ipynb) for detail. Both describe() and summary() are Spark trasnformations. Therefore, they do not produce result immediately. Hence we need show() at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('Survived', 'Pclass', 'Age', 'Fare').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() acts differently in Pandas and Spark. In Spark, it gives the total number of rows in the dataframe. There is no direct way to find the shape of the dataframe. We can use the following trick.  Here count and columns are action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: \\t', df1.count())\n",
    "print('Number of columns: \\t', len(df1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis \n",
    "\n",
    "### About the data visualization in the Spark  \n",
    "\n",
    "There is no native visualization library in Spark. But we can do the lazy transformation on the dataframe, extract the necessary numbers, and make the visualizations out of that. The implementation of this idea can be found [here](https://github.com/roshankoirala/pySpark_tutorial/blob/master/Data_visualization_in_pySpark%20.ipynb). \n",
    "\n",
    "There are options to make visualization by extending other libraries though. However, we do not go to that route here. We will focus on tabular visualization. Tabular visualization is not a bad option. \n",
    "\n",
    "### How many people survived?\n",
    "\n",
    "We can use groupby() and count() transformations to do that. Both are lazy transformation. Again remember that the Spark transformation alone don't evaluate things unless we call an action upon them. Here show() is an action to print the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continious variables\n",
    "\n",
    "Among the features, Fare and Age are the continuous variables (non-categorical). We can inspect them closely here. Our interest would be to find average fare and age. We already use summary() to calculate mean. If we just want mean we can either to summary('mean') or we can also directly call mean() and select columns inside that. Also in summary we can pass multiple arguments like df.summary('mean', 'stddev') and so on. Again please refer to [this](https://github.com/roshankoirala/pySpark_tutorial/blob/master/Exploratory_data_analysis_with_pySpark.ipynb) link for detail. Here groupby() and mean() are Spark transformation. Now you probably started to figure out which is transformation and which is action. I will stop iterating that in every single case now on. \n",
    "\n",
    "Passenger paying more money for the fair is likely to survive than those paying less. This variable might have collinearity with the passenger class that we investigate later. Age seems to be not that important for survival compared to fare.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').mean('Fare', 'Age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "\n",
    "Here we see how each of the categorical variables has affected the survival of the passenger. Spark dataframe also has a pivot() method very similar to the Pandas dataframe to perform this task. \n",
    "\n",
    "Below we see that sex is an (probably the most) important factor for survival. The survival ratio of the female is much higher than that of male. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').pivot('Sex').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the first-class passengers are more likely to survive than the second class. And the third class passengers had very hard luck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').pivot('Pclass').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of siblings and the number of parents also play some role in their survival. The large family is less likely to survive. Similarly, the person with no companion is also less likely to survive. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').pivot('SibSp').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').pivot('Parch').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embark also seems to be important. But it can be collinear with Pclass and fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupBy('Survived').pivot('Embarked').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering \n",
    "\n",
    "First, let's see if there are any missed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df1.columns:\n",
    "    print(col.ljust(20), df1.filter(df1[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many Cabin info is missing. The Cabin is related to Pclass. We will drop this feature. So no problem so far. There are, 2 entries of Embarked missing. We will fill it with the most repeated value S. Age of many people is missing. Again the simplest way to impute the age would be to fill by the average. We choose median for fare imputation. We use Spark's fillna() method to do that. For age we use more complex imputation method discussed below. For now I am just focusing on the train data. There can be different feature missing in the test data. Acutally there is missed fair in test data. So we calculate median fair also. We come to the test data at the end of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('Fare', 'Embarked').summary('mean', '50%', 'max').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.fillna({'Embarked': 'S', 'Fare':14.45})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea for age imputation is to take the title of the people from the name column and impute with the average age of the group of people with that title. Mrs tend to be older than Miss. This method originally appeared in [this](https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83) kernel. We will present the pySpark version of the implementation. \n",
    "\n",
    "First, we extract the title using the regular expression and observe the count and average age with each of the titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('Title', regexp_extract(df1['Name'],\\\n",
    "                '([A-Za-z]+)\\.', 1))\n",
    "\n",
    "df1.groupBy('Title').agg(count('Age'), mean('Age')).sort('count(Age)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that Mr, Miss, and Mrs are highly repeated than other titles. The count of Master is not that high but its average age is much lower than others. So we keep those four titles and map other with one of the first three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dic = {'Mr':'Mr', 'Miss':'Miss', 'Mrs':'Mrs', 'Master':'Master', \\\n",
    "             'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',\\\n",
    "             'Don': 'Mr', 'Mme': 'Miss', 'Jonkheer': 'Mr', 'Lady': 'Mrs',\\\n",
    "             'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs', \\\n",
    "             'Dr':'Mr', 'Rev':'Mr'}\n",
    "\n",
    "mapping = create_map([lit(x) for x in chain(*title_dic.items())])\n",
    "\n",
    "df1 = df1.withColumn('Title', mapping[df1['Title']])\n",
    "df1.groupBy('Title').mean('Age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function that imputes the age column with the average age of the group of people having the same name title as theirs. And use it to impute the ages in the next stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_imputer(df, title, age):\n",
    "    \n",
    "    '''This function search for the null in 'Age' column \n",
    "    of the dataframe df. If there is null then it look \n",
    "    for the title and fill the 'Age' with age argument. \n",
    "    If 'Age' is not null, it will keep the same age.  '''\n",
    "    \n",
    "    return df.withColumn('Age', \\\n",
    "                         when((df['Age'].isNull()) & (df['Title']==title), \\\n",
    "                              age).otherwise(df['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = age_imputer(df1, 'Mr', 33.02)\n",
    "df1 = age_imputer(df1, 'Mrs', 35.98)\n",
    "df1 = age_imputer(df1, 'Miss', 21.86)\n",
    "df1 = age_imputer(df1, 'Master', 4.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new column and dropping a column \n",
    "\n",
    "Now we create a new column called FamilySize combining Parch and SibSp. This API is significantly different in Spark than in Pandas. We use withColumn() method to do that. The first input in the method is a string of the name of the new column. This creates a new column and also keeps the old columns. We will drop the Parch and SibSp column afterward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('FamilySize', df1['Parch'] + df1['SibSp']).\\\n",
    "            drop('Parch', 'SibSp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And drop the unwanted columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop('PassengerID', 'Cabin', 'Name', 'Ticket', 'Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trimmed dataframe. For the small size dataframe show(n) method is not that worse than Pandas head(). See that Sex and Embarked columns are strings. We have to convert them to numeric categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is no missing value now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df1.columns:\n",
    "    print(col.ljust(20), df1.filter(df1[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building \n",
    "\n",
    "So far we used Spark dataframe available in Spark SQL for EDA and feature engineering. Now we will use the Spark ML library to do ML tasks. We will cover the following ML task on Spark ML here:\n",
    "\n",
    "- StringIndexer: Converts string categories to numerical categories. \n",
    "- Vector Assembler: Special to Spark API. We will find detail shortly. \n",
    "- Logistic regression based on Ridge and Lasso regularization. \n",
    "- Tree-based ensemble methods: Random forest and Gradient boosting. \n",
    "- Pipeline: It is a big deal for big data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression,\\\n",
    "                    RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Indexer \n",
    "\n",
    "We will convert the Sex and Embarked column from string to numeric index. This creates a new column for numeric leaving the original intact. So we will remove them afterward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndex = StringIndexer(inputCols=['Sex', 'Embarked'], \n",
    "                       outputCols=['SexNum', 'EmbNum'])\n",
    "\n",
    "stringIndex_model = stringIndex.fit(df1)\n",
    "\n",
    "df1_ = stringIndex_model.transform(df1).drop('Sex', 'Embarked')\n",
    "df1_.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is VectorAssembler?\n",
    "\n",
    "In Python's scikit learn API the model takes X and y variable in the separation matrix. The target y is usually a column vector and feature X is a matrix. scikit learn accepts X as a matrix of dataframe directly. But Spark API is different here. First, it requires X and y in a single matrix instead of two for the training data. It accepts X only in the prediction part as it should. And also X should be a vector in each row (see the output below) of the dataframe. In short, we can not directly feed the dataframe in the model. We should do what VectorAssembler does. \n",
    "\n",
    "In below, inputCols are the feature columns that are doing to be merged to make a vector in each row and outputCol is the name of the merged column. This is the column that Spark ML identifies as the feature column. It is a common practice to rename this as features as Spark ML identifies this name. If its name is different you have to mention column name when fitting model. Then we can select only the feature column and y column. See the illustration below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_asmbl = VectorAssembler(inputCols=df1_.columns[1:], \n",
    "                           outputCol='features')\n",
    "\n",
    "df1_ = vec_asmbl.transform(df1_).select('features', 'Survived')\n",
    "df1_.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the training data into the train and validation part. We split the data into a 7:3 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = df1_.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output form [0] in the bracket can be confusing. 5, [0] means five consecutive entries are zero. Split has not split the data column-wise.\n",
    "\n",
    "\n",
    "### Linear model \n",
    "\n",
    "We study logistic regression here. Spark ML offers elastic net regularization by default. The regularization function is given by \n",
    "\n",
    "$$ \\alpha (\\lambda | {\\bf{w}} |_1) + (1 - \\alpha) \\left(\\frac\\lambda2 |{\\bf{w}}|_2^2 \\right) $$\n",
    "\n",
    "In spark API $\\alpha$ is eleasticNetParam and $\\lambda$ is regParam. We can make our model Ridge by choosing $\\alpha=0$ and Lasso by choosing $\\alpha=1$. \n",
    "\n",
    "Please note that we need to specify the label column at this stage. It the feature column was named differently we had to specify that too here. In Spark, we fit() the model similar to scikit learn but unlike scikit learn we need to name the fitted instance (see comment below). Then we have unified function evaluate() and we call evaluation parameters like predict, accuracy on the evaluation instance. \n",
    "\n",
    "\n",
    "\n",
    "### Evaluation and metric \n",
    "\n",
    "\n",
    "First, we instantiate MulticlassClassificationEvaluator(). We need to specify the metric we want to evaluate at this stage, like metricName='accuracy' in our case. Fitting and evaluating models follow similarly from there. There is an alternative way to evaluate the accuracy scores in the linear model with the following set of commands: \n",
    "\n",
    "- model_name = model.fit(data) \n",
    "- pred = model_name.evaluate(data)\n",
    "- pred.accuracy\n",
    "\n",
    "In this method, there is no need to import MulticlassClassificationEvaluator(). But we stick with the first convention as it provides uniform API for all the models. \n",
    "\n",
    "\n",
    "At this point, I will remind the discussion of transformation and action again. All the machine learning models and preprocessing modules in Spark are transformations. They are evaluated lazily. When we ask for prediction of a model or score of the model then these are Spark action. For example MulticlassClassificationEvaluator() is a transformation while evaluate() is an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', \n",
    "                                          metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = LogisticRegression(labelCol='Survived', \n",
    "                        maxIter=100, \n",
    "                        elasticNetParam=0, # Ridge regression is choosen \n",
    "                        regParam=0.03)\n",
    "\n",
    "model = ridge.fit(train_df)\n",
    "pred = model.transform(valid_df)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LogisticRegression(labelCol='Survived', \n",
    "                           maxIter=100,\n",
    "                           elasticNetParam=1, # Lasso\n",
    "                           regParam=0.0003)\n",
    "\n",
    "model = lasso.fit(train_df)\n",
    "pred = model.transform(valid_df)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, Lasso performs better than Ridge. \n",
    "\n",
    "\n",
    "### Ensemble Tree \n",
    "\n",
    "\n",
    "Currently Spark ML supports two types of ensemble algorithm. Random forest for bagging and gradient boosting for boosting. There is no stacking algorithm available in Spark ML yet. Here we will study both availabel ensemble method both are tree-based methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='Survived', \n",
    "                           numTrees=100, maxDepth=3)\n",
    "\n",
    "model = rf.fit(train_df)\n",
    "pred = model.transform(valid_df)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GBTClassifier(labelCol='Survived', maxIter=75, maxDepth=3)\n",
    "\n",
    "model = gb.fit(train_df)\n",
    "pred = model.transform(valid_df)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the tree-based ensemble method performs better than the linear model and among the tree-based model, gradient boosting performs better than random forest. We may test different method for our final submission.  \n",
    "\n",
    "\n",
    "# Prediction \n",
    "\n",
    "Now we focus on making a prediction on test data and submit the result. We need to follow the exact same procedure for the test data for data cleaning. First we observer the header and see if there are any missing values in the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df2.columns:\n",
    "    print(col.ljust(20), df2.filter(df2[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the train data, there is no missing value in the Embarked column but there is one missing value for fair. And there are few ages missing. Now we fill the missing value by the median fair (of train data, not the test data). We ignore other missing values as we are dropping Cabin from our model. First, we will make a family size feature and drop the unwanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.fillna({'Embarked': 'S', 'Fare':14.45})\n",
    "df2 = df2.withColumn('FamilySize', df2['Parch'] + df2['SibSp']).\\\n",
    "            drop('Parch', 'SibSp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to imputing missing age in the test data. We need to follow exactly the same stages as we did in the train data. Only thing we need to be careful is that we are imputing the averages based on the training data but not the test data. You will identify these steps below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn('Title', regexp_extract(df2['Name'],\\\n",
    "                '([A-Za-z]+)\\.', 1))\n",
    "\n",
    "df2 = df2.withColumn('Title', mapping[df2['Title']])\n",
    "\n",
    "df2.groupBy('Title').agg(count('Age'), mean('Age')).sort('count(Age)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = age_imputer(df2, 'Mr', 33.02)\n",
    "df2 = age_imputer(df2, 'Mrs', 35.98)\n",
    "df2 = age_imputer(df2, 'Miss', 21.86)\n",
    "df2 = age_imputer(df2, 'Master', 4.75)\n",
    "\n",
    "df2 = df2.drop('Cabin', 'Name', 'Ticket', 'Title') # keep PassengerId \n",
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the missing values again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df2.columns:\n",
    "    print(col.ljust(20), df2.filter(df2[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "At this stage, it is worth introducing pipeline. In machine learning, it is common to run a sequence of algorithms to process and learn from data. In our example, we performed StringIndexer, VectorAssembler, and ML model. In other cases, the intermediate stages can be standardization, vectorization (for text processing), normalization, etc. These operations have to be performed on a specific order. Spark represents such a workflow as a Pipeline, which consists of a sequence of stages to be run in a specific order. Pipeline chains multiple Transformers and Estimators together to specify an ML workflow. \n",
    "\n",
    "Without the pipeline, we have to execute each stage, store the outcome, and feed into the next stage and evaluate, and so on. We prefer pipeline over this manual approach because of the following reasons: \n",
    "\n",
    "- The pipeline is less prone to mistake because the processes are automated. \n",
    "- In a production environment, this is the only way to do machine learning end to end. \n",
    "- Pipeline enhances the lazy evaluation. So this is a very natural choice in Spark. The pipeline is even more important for big data.\n",
    "\n",
    "\n",
    "### Grid-search and cross-validation \n",
    "\n",
    "Usually, there are many hyperparameters in a model of selection and some combination of those parameters might give the best result. Tuning them requires checking all possible combinations of the hyperparameter. Doing them manually is a tedious bookkeeping task. Fortunately, there is a grid search option available in Spark like in Sci-kit learn. \n",
    "\n",
    "When doing the grid search we need to validate the model using a separate dataset that was not used to train the data. So far we used customized validation set for comparison between different models. Usually, Spark would be handling very big data. For big data, the train-validation split can be sufficient.  For small datasets like this, however, cross-validation is preferred over the train-validation split. Coss-validation is available in Spark. We will use five-fold cross-validation for better model selection. \n",
    "\n",
    "We use CrossValidator available in Spark ML for the cross-validation. CrossValidator accepts estimatorParamMaps in which we can pass a grid search object built with ParamGridBuilder which is also available in Spark ML. \n",
    "\n",
    "We have chosen a random forest for our submission model. We test three hyperparameters from the random forest: number of trees, minimum information gain in each split. The tuning of the number of trees is not that tricky, higher is better. The only concern here is time it takes for a large number of trees taken. The depth of the tree should be tuned properly. Larger depth with some non-zero info gain can give the best performance. Other objects in the model pipeline have no hyperparameters. If they would we could make a grid using those as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[stringIndex, vec_asmbl, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder().\\\n",
    "            addGrid(rf.maxDepth, [3, 4, 5]).\\\n",
    "            addGrid(rf.minInfoGain, [0., 0.01, 0.1]).\\\n",
    "            addGrid(rf.numTrees, [1000]).\\\n",
    "            build()\n",
    "\n",
    "selected_model = CrossValidator(estimator=pipeline_rf, \n",
    "                                estimatorParamMaps=paramGrid, \n",
    "                                evaluator=evaluator, \n",
    "                                numFolds=5)\n",
    "\n",
    "model_final = selected_model.fit(df1)\n",
    "pred_train = model_final.transform(df1)\n",
    "evaluator.evaluate(pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the in-sample accuracy which is generally higher than the out-sample accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model_final.transform(df2)\n",
    "\n",
    "predictions = pred_test.select('PassengerId', 'prediction')\n",
    "predictions = predictions.\\\n",
    "                withColumn('Survived', predictions['prediction'].\\\n",
    "                cast('integer')).drop('prediction')\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the method to read csv file in Spark. We can even read the csv file using Spark API. But there is some problem with that, especially Pandas can not read that csv and submission through kernel does not work for it. For this reason we change the submission file to pandas and make a submission.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing csv file in Spark \n",
    "predictions.coalesce(1).write.csv('submission_file.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the saved file from spark \n",
    "spark.read.csv('submission_file.csv', header=True).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing csv file using Pandas \n",
    "predictions.toPandas().to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting csv file in pandas \n",
    "import pandas as pd\n",
    "pd.read_csv('submission.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the model itself for future use so that you don't have to train every time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.write().save('titanic_classification.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls titanic_classification.model/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thought. \n",
    "\n",
    "- We just presented a base model here and established that Spark is basically capable of doing many tasks on machine learning and model building workflow seamlessly. The model is not fine-tuned yet. It would be interesting to see Spark matching Sci-kit learn's performance. \n",
    "\n",
    "- We did not perform standardization. The reason is standardization is inbuilt in the Spark linear model and it is not needed for the tree-based models. If we had chosen a linear model as our prediction model we may have to turn it off in order to make normalization based on train data instead of test data while making a prediction. \n",
    "\n",
    "- We did not include everything in the pipeline. For example, we imputed null values outside the pipeline. In a production environment, it is required to have everything in the pipeline. So there is room for improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
