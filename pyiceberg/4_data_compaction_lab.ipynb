{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Compaction & File Management Lab - Apache Iceberg\n",
        "\n",
        "## ğŸ¯ Lab Objectives\n",
        "\n",
        "In this lab, we will explore Apache Iceberg's powerful data compaction and file management capabilities:\n",
        "\n",
        "1. **Small File Problem**: Understand and demonstrate the small file problem\n",
        "2. **Compaction Strategies**: Implement different compaction approaches\n",
        "3. **File Size Optimization**: Analyze and optimize file sizes\n",
        "4. **Storage Efficiency**: Improve storage costs and query performance\n",
        "5. **Compaction Policies**: Configure automatic compaction rules\n",
        "6. **Real-world Scenarios**: Apply compaction to realistic datasets\n",
        "\n",
        "## ğŸ—ï¸ Compaction Architecture\n",
        "\n",
        "### Iceberg Compaction Types:\n",
        "- **Rewrite Compaction**: Merge small files into larger ones\n",
        "- **Bin Packing**: Optimize file sizes for better performance\n",
        "- **Sort Compaction**: Sort data within files for optimal access\n",
        "- **Z-Order Compaction**: Optimize for multi-dimensional queries\n",
        "\n",
        "### Performance Benefits:\n",
        "- **Reduced File Count**: Fewer files to scan\n",
        "- **Better Query Performance**: Larger files = better I/O\n",
        "- **Storage Efficiency**: Reduced metadata overhead\n",
        "- **Cost Reduction**: Lower storage and compute costs\n",
        "\n",
        "## ğŸ“Š Dataset: E-commerce Transaction Data\n",
        "\n",
        "We will work with comprehensive e-commerce data including:\n",
        "- **Sales Transactions**: Time-series sales data\n",
        "- **Product Catalog**: Hierarchical product information\n",
        "- **Customer Data**: Geographic and demographic data\n",
        "- **File Management**: Small file creation and compaction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Successfully imported all libraries!\n",
            "ğŸ“¦ PyArrow version: 21.0.0\n",
            "ğŸ“¦ Pandas version: 2.3.2\n",
            "ğŸ“¦ NumPy version: 2.2.6\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# PyIceberg imports\n",
        "from pyiceberg.catalog import load_catalog\n",
        "from pyiceberg.schema import Schema\n",
        "from pyiceberg.types import (\n",
        "    StructType, StringType, IntegerType, LongType, DoubleType, BooleanType,\n",
        "    TimestampType, DateType, NestedField\n",
        ")\n",
        "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
        "from pyiceberg.transforms import IdentityTransform\n",
        "\n",
        "# Data processing\n",
        "import pyarrow as pa\n",
        "import pyarrow.compute as pc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Performance monitoring\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "print(\"âœ… Successfully imported all libraries!\")\n",
        "print(f\"ğŸ“¦ PyArrow version: {pa.__version__}\")\n",
        "print(f\"ğŸ“¦ Pandas version: {pd.__version__}\")\n",
        "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Warehouse and Catalog Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Created namespace 'ecommerce'\n",
            "ğŸ“ Warehouse path: /tmp/compaction_iceberg_warehouse\n",
            "ğŸ¯ Ready for Data Compaction Lab!\n"
          ]
        }
      ],
      "source": [
        "# Setup warehouse and catalog\n",
        "warehouse_path = \"/tmp/compaction_iceberg_warehouse\"\n",
        "os.makedirs(warehouse_path, exist_ok=True)\n",
        "\n",
        "# Configure catalog\n",
        "catalog = load_catalog(\n",
        "    \"compaction\",\n",
        "    **{\n",
        "        'type': 'sql',\n",
        "        \"uri\": f\"sqlite:///{warehouse_path}/compaction_catalog.db\",\n",
        "        \"warehouse\": f\"file://{warehouse_path}\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Create namespace\n",
        "try:\n",
        "    catalog.create_namespace(\"ecommerce\")\n",
        "    print(\"âœ… Created namespace 'ecommerce'\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸  Namespace 'ecommerce' already exists: {e}\")\n",
        "\n",
        "print(f\"ğŸ“ Warehouse path: {warehouse_path}\")\n",
        "print(\"ğŸ¯ Ready for Data Compaction Lab!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate E-commerce Dataset for Compaction Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Generating sales data for compaction testing...\n",
            "âœ… Generated 100,000 sales transactions\n",
            "\n",
            "ğŸ“‹ Sample transaction data:\n",
            "  transaction_id: TXN_000001\n",
            "  sale_date: 2023-04-12\n",
            "  sale_timestamp: 2023-04-12 00:00:00\n",
            "  customer_id: CUST_00151\n",
            "  product_id: PROD_00042\n",
            "  product_name: Product 1652\n",
            "  category: Sports\n",
            "  brand: Canon\n",
            "  region: North America\n",
            "  country: Germany\n",
            "  quantity: 5\n",
            "  unit_price: 986.89\n",
            "  total_amount: 4161.22\n",
            "  discount_percent: 15.67\n",
            "  payment_method: Credit Card\n",
            "  shipping_method: Express\n",
            "  customer_segment: Budget\n",
            "  is_returned: False\n",
            "  return_date: None\n",
            "  sales_rep_id: REP_006\n"
          ]
        }
      ],
      "source": [
        "def generate_sales_data(n_transactions=100000):\n",
        "    \"\"\"Generate realistic e-commerce sales data for compaction experiments\"\"\"\n",
        "    \n",
        "    transactions = []\n",
        "    categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home\", \"Sports\"]\n",
        "    brands = [\"Apple\", \"Samsung\", \"Nike\", \"Adidas\", \"Sony\", \"LG\", \"Canon\", \"Dell\"]\n",
        "    regions = [\"North America\", \"Europe\", \"Asia Pacific\", \"Latin America\"]\n",
        "    countries = [\"USA\", \"Canada\", \"UK\", \"Germany\", \"Japan\", \"Australia\", \"Brazil\", \"Mexico\"]\n",
        "    payment_methods = [\"Credit Card\", \"Debit Card\", \"PayPal\", \"Apple Pay\", \"Google Pay\"]\n",
        "    shipping_methods = [\"Standard\", \"Express\", \"Overnight\", \"Same Day\"]\n",
        "    customer_segments = [\"Premium\", \"Standard\", \"Budget\", \"VIP\"]\n",
        "    \n",
        "    # Generate data over 6 months\n",
        "    start_date = datetime(2023, 1, 1)\n",
        "    end_date = datetime(2023, 6, 30)\n",
        "    \n",
        "    for i in range(n_transactions):\n",
        "        # Generate random date within range\n",
        "        days_diff = (end_date - start_date).days\n",
        "        random_days = random.randint(0, days_diff)\n",
        "        sale_date = start_date + timedelta(days=random_days)\n",
        "        \n",
        "        # Ensure microsecond precision for Iceberg compatibility\n",
        "        sale_date = sale_date.replace(microsecond=sale_date.microsecond)\n",
        "        \n",
        "        transaction = {\n",
        "            \"transaction_id\": f\"TXN_{i+1:06d}\",\n",
        "            \"sale_date\": sale_date.strftime(\"%Y-%m-%d\"),\n",
        "            \"sale_timestamp\": sale_date,\n",
        "            \"customer_id\": f\"CUST_{random.randint(1, 10000):05d}\",\n",
        "            \"product_id\": f\"PROD_{random.randint(1, 5000):05d}\",\n",
        "            \"product_name\": f\"Product {random.randint(1, 5000)}\",\n",
        "            \"category\": random.choice(categories),\n",
        "            \"brand\": random.choice(brands),\n",
        "            \"region\": random.choice(regions),\n",
        "            \"country\": random.choice(countries),\n",
        "            \"quantity\": random.randint(1, 10),\n",
        "            \"unit_price\": round(random.uniform(10, 1000), 2),\n",
        "            \"total_amount\": 0,  # Will calculate below\n",
        "            \"discount_percent\": round(random.uniform(0, 30), 2),\n",
        "            \"payment_method\": random.choice(payment_methods),\n",
        "            \"shipping_method\": random.choice(shipping_methods),\n",
        "            \"customer_segment\": random.choice(customer_segments),\n",
        "            \"is_returned\": random.choice([True, False]),\n",
        "            \"return_date\": None,\n",
        "            \"sales_rep_id\": f\"REP_{random.randint(1, 100):03d}\"\n",
        "        }\n",
        "        \n",
        "        # Calculate total amount with discount\n",
        "        subtotal = transaction[\"quantity\"] * transaction[\"unit_price\"]\n",
        "        discount_amount = subtotal * (transaction[\"discount_percent\"] / 100)\n",
        "        transaction[\"total_amount\"] = round(subtotal - discount_amount, 2)\n",
        "        \n",
        "        # Add return date if item was returned\n",
        "        if transaction[\"is_returned\"]:\n",
        "            return_days = random.randint(1, 30)\n",
        "            return_date = sale_date + timedelta(days=return_days)\n",
        "            transaction[\"return_date\"] = return_date.strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "        transactions.append(transaction)\n",
        "    \n",
        "    return transactions\n",
        "\n",
        "# Generate the dataset\n",
        "print(\"ğŸ”„ Generating sales data for compaction testing...\")\n",
        "sales_data = generate_sales_data(100000)  # 100K records\n",
        "print(f\"âœ… Generated {len(sales_data):,} sales transactions\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nğŸ“‹ Sample transaction data:\")\n",
        "sample_transaction = sales_data[0]\n",
        "for key, value in sample_transaction.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Schema and Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—ï¸ Creating sales schema...\n",
            "âœ… Schema created successfully!\n",
            "âœ… Helper functions created!\n"
          ]
        }
      ],
      "source": [
        "# Create Iceberg schema for sales data\n",
        "def create_sales_schema():\n",
        "    \"\"\"Create Iceberg schema for sales transactions\"\"\"\n",
        "    \n",
        "    schema = Schema(\n",
        "        NestedField(1, \"transaction_id\", StringType(), required=True),\n",
        "        NestedField(2, \"sale_date\", DateType(), required=True),\n",
        "        NestedField(3, \"sale_timestamp\", TimestampType(), required=True),\n",
        "        NestedField(4, \"customer_id\", StringType(), required=True),\n",
        "        NestedField(5, \"product_id\", StringType(), required=True),\n",
        "        NestedField(6, \"product_name\", StringType(), required=True),\n",
        "        NestedField(7, \"category\", StringType(), required=True),\n",
        "        NestedField(8, \"brand\", StringType(), required=True),\n",
        "        NestedField(9, \"region\", StringType(), required=True),\n",
        "        NestedField(10, \"country\", StringType(), required=True),\n",
        "        NestedField(11, \"quantity\", IntegerType(), required=True),\n",
        "        NestedField(12, \"unit_price\", DoubleType(), required=True),\n",
        "        NestedField(13, \"total_amount\", DoubleType(), required=True),\n",
        "        NestedField(14, \"discount_percent\", DoubleType(), required=True),\n",
        "        NestedField(15, \"payment_method\", StringType(), required=True),\n",
        "        NestedField(16, \"shipping_method\", StringType(), required=True),\n",
        "        NestedField(17, \"customer_segment\", StringType(), required=True),\n",
        "        NestedField(18, \"is_returned\", BooleanType(), required=True),\n",
        "        NestedField(19, \"return_date\", StringType(), required=False),  # Can be null\n",
        "        NestedField(20, \"sales_rep_id\", StringType(), required=True)\n",
        "    )\n",
        "    \n",
        "    return schema\n",
        "\n",
        "# Create the schema\n",
        "print(\"ğŸ—ï¸ Creating sales schema...\")\n",
        "sales_schema = create_sales_schema()\n",
        "print(\"âœ… Schema created successfully!\")\n",
        "\n",
        "# Helper function to create PyArrow table with correct timestamp precision\n",
        "def create_pyarrow_table_with_timestamps(df):\n",
        "    \"\"\"Create PyArrow table with microsecond timestamp precision and correct nullable settings\"\"\"\n",
        "    \n",
        "    # Create PyArrow schema that matches Iceberg schema\n",
        "    pyarrow_schema = pa.schema([\n",
        "        pa.field('transaction_id', pa.string(), nullable=False),\n",
        "        pa.field('sale_date', pa.date32(), nullable=False),\n",
        "        pa.field('sale_timestamp', pa.timestamp('us'), nullable=False),\n",
        "        pa.field('customer_id', pa.string(), nullable=False),\n",
        "        pa.field('product_id', pa.string(), nullable=False),\n",
        "        pa.field('product_name', pa.string(), nullable=False),\n",
        "        pa.field('category', pa.string(), nullable=False),\n",
        "        pa.field('brand', pa.string(), nullable=False),\n",
        "        pa.field('region', pa.string(), nullable=False),\n",
        "        pa.field('country', pa.string(), nullable=False),\n",
        "        pa.field('quantity', pa.int32(), nullable=False),\n",
        "        pa.field('unit_price', pa.float64(), nullable=False),\n",
        "        pa.field('total_amount', pa.float64(), nullable=False),\n",
        "        pa.field('discount_percent', pa.float64(), nullable=False),\n",
        "        pa.field('payment_method', pa.string(), nullable=False),\n",
        "        pa.field('shipping_method', pa.string(), nullable=False),\n",
        "        pa.field('customer_segment', pa.string(), nullable=False),\n",
        "        pa.field('is_returned', pa.bool_(), nullable=False),\n",
        "        pa.field('return_date', pa.string(), nullable=True),  # This one can be null\n",
        "        pa.field('sales_rep_id', pa.string(), nullable=False)\n",
        "    ])\n",
        "    \n",
        "    # Convert timestamp to microseconds (Iceberg requirement)\n",
        "    df_clean = df.copy()\n",
        "    df_clean['sale_timestamp'] = pd.to_datetime(df_clean['sale_timestamp']).dt.floor('us')\n",
        "    df_clean['sale_date'] = pd.to_datetime(df_clean['sale_date']).dt.date\n",
        "    \n",
        "    # Create table with explicit schema\n",
        "    return pa.table({\n",
        "        'transaction_id': df_clean['transaction_id'],\n",
        "        'sale_date': df_clean['sale_date'],\n",
        "        'sale_timestamp': df_clean['sale_timestamp'],\n",
        "        'customer_id': df_clean['customer_id'],\n",
        "        'product_id': df_clean['product_id'],\n",
        "        'product_name': df_clean['product_name'],\n",
        "        'category': df_clean['category'],\n",
        "        'brand': df_clean['brand'],\n",
        "        'region': df_clean['region'],\n",
        "        'country': df_clean['country'],\n",
        "        'quantity': df_clean['quantity'],\n",
        "        'unit_price': df_clean['unit_price'],\n",
        "        'total_amount': df_clean['total_amount'],\n",
        "        'discount_percent': df_clean['discount_percent'],\n",
        "        'payment_method': df_clean['payment_method'],\n",
        "        'shipping_method': df_clean['shipping_method'],\n",
        "        'customer_segment': df_clean['customer_segment'],\n",
        "        'is_returned': df_clean['is_returned'],\n",
        "        'return_date': df_clean['return_date'],\n",
        "        'sales_rep_id': df_clean['sales_rep_id']\n",
        "    }, schema=pyarrow_schema)\n",
        "\n",
        "# Helper function to analyze file statistics\n",
        "def analyze_file_statistics(table, table_name):\n",
        "    \"\"\"Analyze file statistics for a table\"\"\"\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Analyzing file statistics for {table_name}:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    files_info = table.inspect.files()\n",
        "    if len(files_info) > 0:\n",
        "        files_df = files_info.to_pandas()\n",
        "        \n",
        "        # Basic statistics\n",
        "        total_files = len(files_df)\n",
        "        total_size = files_df['file_size_in_bytes'].sum()\n",
        "        avg_file_size = files_df['file_size_in_bytes'].mean()\n",
        "        min_file_size = files_df['file_size_in_bytes'].min()\n",
        "        max_file_size = files_df['file_size_in_bytes'].max()\n",
        "        \n",
        "        print(f\"ğŸ“ Total files: {total_files:,}\")\n",
        "        print(f\"ğŸ’¾ Total size: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)\")\n",
        "        print(f\"ğŸ“ Average file size: {avg_file_size:,.0f} bytes ({avg_file_size/1024:.2f} KB)\")\n",
        "        print(f\"ğŸ“ Min file size: {min_file_size:,} bytes ({min_file_size/1024:.2f} KB)\")\n",
        "        print(f\"ğŸ“ Max file size: {max_file_size:,} bytes ({max_file_size/1024:.2f} KB)\")\n",
        "        \n",
        "        # File size distribution\n",
        "        small_files = len(files_df[files_df['file_size_in_bytes'] < 64*1024])  # < 64KB\n",
        "        medium_files = len(files_df[(files_df['file_size_in_bytes'] >= 64*1024) & \n",
        "                                   (files_df['file_size_in_bytes'] < 1024*1024)])  # 64KB - 1MB\n",
        "        large_files = len(files_df[files_df['file_size_in_bytes'] >= 1024*1024])  # >= 1MB\n",
        "        \n",
        "        print(f\"\\nğŸ“Š File size distribution:\")\n",
        "        print(f\"  Small files (< 64KB): {small_files:,} ({small_files/total_files*100:.1f}%)\")\n",
        "        print(f\"  Medium files (64KB-1MB): {medium_files:,} ({medium_files/total_files*100:.1f}%)\")\n",
        "        print(f\"  Large files (>= 1MB): {large_files:,} ({large_files/total_files*100:.1f}%)\")\n",
        "        \n",
        "        return {\n",
        "            'total_files': total_files,\n",
        "            'total_size': total_size,\n",
        "            'avg_file_size': avg_file_size,\n",
        "            'min_file_size': min_file_size,\n",
        "            'max_file_size': max_file_size,\n",
        "            'small_files': small_files,\n",
        "            'medium_files': medium_files,\n",
        "            'large_files': large_files\n",
        "        }\n",
        "    else:\n",
        "        print(\"âŒ No files found in table\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… Helper functions created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Small File Problem Demonstration\n",
        "\n",
        "The **Small File Problem** is a common issue in data lakes where many small files are created instead of fewer, larger files. This leads to:\n",
        "\n",
        "### ğŸš¨ **Problems with Small Files:**\n",
        "- **Poor Query Performance**: More files to scan = slower queries\n",
        "- **High Metadata Overhead**: Each file has metadata overhead\n",
        "- **Storage Inefficiency**: Small files waste storage space\n",
        "- **High Costs**: More API calls and compute resources needed\n",
        "\n",
        "### ğŸ“Š **Let's Create Small Files Intentionally:**\n",
        "We'll create a table with many small files by inserting data in small batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Creating table with small files...\n",
            "â„¹ï¸ No existing table to drop: Table does not exist: ecommerce.sales_small_files\n",
            "ğŸ“¦ Inserting data in 100 batches of 1000 records each...\n",
            "  âœ… Inserted batch 10/100\n",
            "  âœ… Inserted batch 20/100\n",
            "  âœ… Inserted batch 30/100\n",
            "  âœ… Inserted batch 40/100\n",
            "  âœ… Inserted batch 50/100\n",
            "  âœ… Inserted batch 60/100\n",
            "  âœ… Inserted batch 70/100\n",
            "  âœ… Inserted batch 80/100\n",
            "  âœ… Inserted batch 90/100\n",
            "  âœ… Inserted batch 100/100\n",
            "âœ… Created table with small files!\n",
            "\n",
            "ğŸ“Š Analyzing file statistics for Small Files Table:\n",
            "--------------------------------------------------\n",
            "ğŸ“ Total files: 100\n",
            "ğŸ’¾ Total size: 4,229,095 bytes (4.03 MB)\n",
            "ğŸ“ Average file size: 42,291 bytes (41.30 KB)\n",
            "ğŸ“ Min file size: 42,145 bytes (41.16 KB)\n",
            "ğŸ“ Max file size: 42,443 bytes (41.45 KB)\n",
            "\n",
            "ğŸ“Š File size distribution:\n",
            "  Small files (< 64KB): 100 (100.0%)\n",
            "  Medium files (64KB-1MB): 0 (0.0%)\n",
            "  Large files (>= 1MB): 0 (0.0%)\n",
            "\n",
            "ğŸš¨ Small File Problem Analysis:\n",
            "  Total files: 100\n",
            "  Average file size: 42,291 bytes (41.30 KB)\n",
            "  Small files (< 64KB): 100 (100.0%)\n",
            "  âš ï¸  PROBLEM: 100.0% of files are small!\n"
          ]
        }
      ],
      "source": [
        "# Create a table with many small files\n",
        "print(\"ğŸ”„ Creating table with small files...\")\n",
        "\n",
        "# Drop existing table if it exists\n",
        "try:\n",
        "    catalog.drop_table(\"ecommerce.sales_small_files\")\n",
        "    print(\"ğŸ—‘ï¸ Dropped existing table\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸ No existing table to drop: {e}\")\n",
        "\n",
        "# Create unpartitioned table\n",
        "small_files_table = catalog.create_table(\n",
        "    \"ecommerce.sales_small_files\",\n",
        "    schema=sales_schema\n",
        ")\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(sales_data)\n",
        "\n",
        "# Create small files by inserting data in small batches (1000 records each)\n",
        "batch_size = 1000\n",
        "total_batches = len(df) // batch_size\n",
        "\n",
        "print(f\"ğŸ“¦ Inserting data in {total_batches} batches of {batch_size} records each...\")\n",
        "\n",
        "for i in range(total_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(df))\n",
        "    batch_df = df.iloc[start_idx:end_idx]\n",
        "    \n",
        "    # Convert batch to PyArrow table\n",
        "    batch_table = create_pyarrow_table_with_timestamps(batch_df)\n",
        "    \n",
        "    # Insert batch\n",
        "    small_files_table.append(batch_table)\n",
        "    \n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"  âœ… Inserted batch {i + 1}/{total_batches}\")\n",
        "\n",
        "print(f\"âœ… Created table with small files!\")\n",
        "\n",
        "# Analyze the small files problem\n",
        "small_files_stats = analyze_file_statistics(small_files_table, \"Small Files Table\")\n",
        "\n",
        "# Show the problem\n",
        "if small_files_stats:\n",
        "    print(f\"\\nğŸš¨ Small File Problem Analysis:\")\n",
        "    print(f\"  Total files: {small_files_stats['total_files']:,}\")\n",
        "    print(f\"  Average file size: {small_files_stats['avg_file_size']:,.0f} bytes ({small_files_stats['avg_file_size']/1024:.2f} KB)\")\n",
        "    print(f\"  Small files (< 64KB): {small_files_stats['small_files']:,} ({small_files_stats['small_files']/small_files_stats['total_files']*100:.1f}%)\")\n",
        "    \n",
        "    if small_files_stats['small_files'] > small_files_stats['total_files'] * 0.8:\n",
        "        print(f\"  âš ï¸  PROBLEM: {small_files_stats['small_files']/small_files_stats['total_files']*100:.1f}% of files are small!\")\n",
        "    else:\n",
        "        print(f\"  âœ… File sizes are reasonable\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compaction Strategies\n",
        "\n",
        "Now let's implement different compaction strategies to solve the small file problem:\n",
        "\n",
        "### ğŸ”§ **Compaction Strategies:**\n",
        "\n",
        "1. **Rewrite Compaction**: Merge small files into larger ones\n",
        "2. **Bin Packing**: Optimize file sizes for better performance  \n",
        "3. **Sort Compaction**: Sort data within files for optimal access\n",
        "4. **Z-Order Compaction**: Optimize for multi-dimensional queries\n",
        "\n",
        "### ğŸ“Š **Let's Implement Rewrite Compaction:**\n",
        "This is the most common and effective compaction strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Implementing Rewrite Compaction...\n",
            "â„¹ï¸ No existing compacted table to drop: Table does not exist: ecommerce.sales_compacted\n",
            "ğŸ“¦ Inserting all data at once to create larger files...\n",
            "âœ… Created compacted table!\n",
            "\n",
            "ğŸ“Š Analyzing file statistics for Compacted Table:\n",
            "--------------------------------------------------\n",
            "ğŸ“ Total files: 1\n",
            "ğŸ’¾ Total size: 2,653,530 bytes (2.53 MB)\n",
            "ğŸ“ Average file size: 2,653,530 bytes (2591.34 KB)\n",
            "ğŸ“ Min file size: 2,653,530 bytes (2591.34 KB)\n",
            "ğŸ“ Max file size: 2,653,530 bytes (2591.34 KB)\n",
            "\n",
            "ğŸ“Š File size distribution:\n",
            "  Small files (< 64KB): 0 (0.0%)\n",
            "  Medium files (64KB-1MB): 0 (0.0%)\n",
            "  Large files (>= 1MB): 1 (100.0%)\n",
            "\n",
            "ğŸ“Š Compaction Results Comparison:\n",
            "------------------------------------------------------------\n",
            "Metric                    Small Files     Compacted       Improvement    \n",
            "------------------------------------------------------------\n",
            "Total Files               100             1               100.0x fewer\n",
            "Avg File Size             41.3          KB 2591.3        KB 62.7x larger\n",
            "Small Files (<64KB)       100             0               infx fewer\n",
            "Storage Efficiency        N/A             N/A             37.3% saved\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ¯ Compaction Summary:\n",
            "  âœ… Reduced files from 100 to 1\n",
            "  âœ… Increased average file size by 62.7x\n",
            "  âœ… Reduced small files by infx\n",
            "  âœ… Improved storage efficiency by 37.3%\n"
          ]
        }
      ],
      "source": [
        "# Implement Rewrite Compaction\n",
        "print(\"ğŸ”„ Implementing Rewrite Compaction...\")\n",
        "\n",
        "# Create a new table for compacted data\n",
        "try:\n",
        "    catalog.drop_table(\"ecommerce.sales_compacted\")\n",
        "    print(\"ğŸ—‘ï¸ Dropped existing compacted table\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸ No existing compacted table to drop: {e}\")\n",
        "\n",
        "# Create compacted table\n",
        "compacted_table = catalog.create_table(\n",
        "    \"ecommerce.sales_compacted\",\n",
        "    schema=sales_schema\n",
        ")\n",
        "\n",
        "# Insert all data at once to create larger files\n",
        "print(\"ğŸ“¦ Inserting all data at once to create larger files...\")\n",
        "\n",
        "# Convert entire dataset to PyArrow table\n",
        "full_table = create_pyarrow_table_with_timestamps(df)\n",
        "\n",
        "# Insert all data in one operation\n",
        "compacted_table.append(full_table)\n",
        "\n",
        "print(\"âœ… Created compacted table!\")\n",
        "\n",
        "# Analyze the compacted table\n",
        "compacted_stats = analyze_file_statistics(compacted_table, \"Compacted Table\")\n",
        "\n",
        "# Compare before and after compaction\n",
        "if small_files_stats and compacted_stats:\n",
        "    print(f\"\\nğŸ“Š Compaction Results Comparison:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Metric':<25} {'Small Files':<15} {'Compacted':<15} {'Improvement':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # File count comparison\n",
        "    file_reduction = small_files_stats['total_files'] / compacted_stats['total_files']\n",
        "    print(f\"{'Total Files':<25} {small_files_stats['total_files']:<15,} {compacted_stats['total_files']:<15,} {file_reduction:.1f}x fewer\")\n",
        "    \n",
        "    # File size comparison\n",
        "    size_increase = compacted_stats['avg_file_size'] / small_files_stats['avg_file_size']\n",
        "    print(f\"{'Avg File Size':<25} {small_files_stats['avg_file_size']/1024:<14.1f}KB {compacted_stats['avg_file_size']/1024:<14.1f}KB {size_increase:.1f}x larger\")\n",
        "    \n",
        "    # Small files comparison\n",
        "    small_files_reduction = small_files_stats['small_files'] / compacted_stats['small_files'] if compacted_stats['small_files'] > 0 else float('inf')\n",
        "    print(f\"{'Small Files (<64KB)':<25} {small_files_stats['small_files']:<15,} {compacted_stats['small_files']:<15,} {small_files_reduction:.1f}x fewer\")\n",
        "    \n",
        "    # Storage efficiency\n",
        "    storage_efficiency = (small_files_stats['total_size'] - compacted_stats['total_size']) / small_files_stats['total_size'] * 100\n",
        "    print(f\"{'Storage Efficiency':<25} {'N/A':<15} {'N/A':<15} {storage_efficiency:.1f}% saved\")\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\nğŸ¯ Compaction Summary:\")\n",
        "    print(f\"  âœ… Reduced files from {small_files_stats['total_files']:,} to {compacted_stats['total_files']:,}\")\n",
        "    print(f\"  âœ… Increased average file size by {size_increase:.1f}x\")\n",
        "    print(f\"  âœ… Reduced small files by {small_files_reduction:.1f}x\")\n",
        "    print(f\"  âœ… Improved storage efficiency by {storage_efficiency:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Testing - Before vs After Compaction\n",
        "\n",
        "Let's test query performance to see the impact of compaction on query speed:\n",
        "\n",
        "### â±ï¸ **Performance Metrics:**\n",
        "- **Query Execution Time**: How fast queries run\n",
        "- **Files Scanned**: Number of files accessed during queries\n",
        "- **Memory Usage**: Memory consumption during queries\n",
        "- **I/O Operations**: Disk read operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â±ï¸ Testing query performance...\n",
            "\n",
            "ğŸ” Testing Small Files Table:\n",
            "  Full Table Scan: 0.182s (100 files)\n",
            "  Date Filter: 0.216s (100 files)\n",
            "  Category Filter: 0.222s (100 files)\n",
            "  Amount Filter: 0.213s (100 files)\n",
            "  Complex Filter: 0.220s (100 files)\n",
            "\n",
            "ğŸ” Testing Compacted Table:\n",
            "  Full Table Scan: 0.008s (1 files)\n",
            "  Date Filter: 0.016s (1 files)\n",
            "  Category Filter: 0.020s (1 files)\n",
            "  Amount Filter: 0.039s (1 files)\n",
            "  Complex Filter: 0.025s (1 files)\n",
            "\n",
            "ğŸ“Š Performance Comparison:\n",
            "--------------------------------------------------------------------------------\n",
            "Query                Small Files (s) Compacted (s)   Speedup         Files Reduction\n",
            "--------------------------------------------------------------------------------\n",
            "Full Table Scan      0.182          0.008          22.3          x 100.0         x\n",
            "Date Filter          0.216          0.016          13.4          x 100.0         x\n",
            "Category Filter      0.222          0.020          11.2          x 100.0         x\n",
            "Amount Filter        0.213          0.039          5.5           x 100.0         x\n",
            "Complex Filter       0.220          0.025          8.8           x 100.0         x\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ¯ Average Improvements:\n",
            "  âš¡ Query Speed: 12.2x faster\n",
            "  ğŸ“ File Reduction: 100.0x fewer files\n",
            "  ğŸ’¾ Storage Efficiency: Better file organization\n",
            "  ğŸš€ Overall Performance: Significantly improved\n"
          ]
        }
      ],
      "source": [
        "# Performance testing function\n",
        "def measure_query_performance(table, query_name, row_filter=None):\n",
        "    \"\"\"Measure query performance for a table\"\"\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    try:\n",
        "        # Execute query\n",
        "        if row_filter:\n",
        "            # Apply filter if provided\n",
        "            result = table.scan(row_filter=row_filter).to_arrow()\n",
        "        else:\n",
        "            # Full table scan\n",
        "            result = table.scan().to_arrow()\n",
        "        \n",
        "        execution_time = time.time() - start_time\n",
        "        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
        "        memory_used = end_memory - start_memory\n",
        "        \n",
        "        # Get file information\n",
        "        files_info = table.inspect.files()\n",
        "        files_scanned = len(files_info) if files_info else 0\n",
        "        \n",
        "        return {\n",
        "            'query_name': query_name,\n",
        "            'execution_time': execution_time,\n",
        "            'records_returned': len(result),\n",
        "            'files_scanned': files_scanned,\n",
        "            'memory_used': memory_used\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error executing query {query_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    (\"Full Table Scan\", None),\n",
        "    (\"Date Filter\", \"sale_date >= '2023-03-01' AND sale_date < '2023-04-01'\"),\n",
        "    (\"Category Filter\", \"category = 'Electronics'\"),\n",
        "    (\"Amount Filter\", \"total_amount > 500\"),\n",
        "    (\"Complex Filter\", \"category = 'Electronics' AND total_amount > 500 AND region = 'North America'\")\n",
        "]\n",
        "\n",
        "print(\"â±ï¸ Testing query performance...\")\n",
        "\n",
        "# Test small files table\n",
        "print(\"\\nğŸ” Testing Small Files Table:\")\n",
        "small_files_results = []\n",
        "\n",
        "for query_name, row_filter in test_queries:\n",
        "    result = measure_query_performance(small_files_table, f\"{query_name} (Small Files)\", row_filter)\n",
        "    if result:\n",
        "        small_files_results.append(result)\n",
        "        print(f\"  {query_name}: {result['execution_time']:.3f}s ({result['files_scanned']} files)\")\n",
        "\n",
        "# Test compacted table\n",
        "print(\"\\nğŸ” Testing Compacted Table:\")\n",
        "compacted_results = []\n",
        "\n",
        "for query_name, row_filter in test_queries:\n",
        "    result = measure_query_performance(compacted_table, f\"{query_name} (Compacted)\", row_filter)\n",
        "    if result:\n",
        "        compacted_results.append(result)\n",
        "        print(f\"  {query_name}: {result['execution_time']:.3f}s ({result['files_scanned']} files)\")\n",
        "\n",
        "# Compare results\n",
        "if small_files_results and compacted_results:\n",
        "    print(f\"\\nğŸ“Š Performance Comparison:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Query':<20} {'Small Files (s)':<15} {'Compacted (s)':<15} {'Speedup':<15} {'Files Reduction':<15}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for i, (small_result, compacted_result) in enumerate(zip(small_files_results, compacted_results)):\n",
        "        speedup = small_result['execution_time'] / compacted_result['execution_time']\n",
        "        file_reduction = small_result['files_scanned'] / compacted_result['files_scanned'] if compacted_result['files_scanned'] > 0 else float('inf')\n",
        "        \n",
        "        print(f\"{test_queries[i][0]:<20} {small_result['execution_time']:<14.3f} {compacted_result['execution_time']:<14.3f} {speedup:<14.1f}x {file_reduction:<14.1f}x\")\n",
        "    \n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Calculate average improvements\n",
        "    avg_speedup = sum(small_result['execution_time'] / compacted_result['execution_time'] \n",
        "                     for small_result, compacted_result in zip(small_files_results, compacted_results)) / len(small_files_results)\n",
        "    avg_file_reduction = sum(small_result['files_scanned'] / compacted_result['files_scanned'] \n",
        "                            for small_result, compacted_result in zip(small_files_results, compacted_results) \n",
        "                            if compacted_result['files_scanned'] > 0) / len(small_files_results)\n",
        "    \n",
        "    print(f\"\\nğŸ¯ Average Improvements:\")\n",
        "    print(f\"  âš¡ Query Speed: {avg_speedup:.1f}x faster\")\n",
        "    print(f\"  ğŸ“ File Reduction: {avg_file_reduction:.1f}x fewer files\")\n",
        "    print(f\"  ğŸ’¾ Storage Efficiency: Better file organization\")\n",
        "    print(f\"  ğŸš€ Overall Performance: Significantly improved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Lab Summary & Best Practices\n",
        "\n",
        "### ğŸ‰ **Congratulations!**\n",
        "\n",
        "You've completed the **Data Compaction & File Management Lab** and learned about:\n",
        "\n",
        "âœ… **Small File Problem**: Understanding the impact of small files  \n",
        "âœ… **Compaction Strategies**: Implementing rewrite compaction  \n",
        "âœ… **File Size Optimization**: Analyzing and improving file sizes  \n",
        "âœ… **Performance Testing**: Measuring query performance improvements  \n",
        "âœ… **Storage Efficiency**: Reducing costs and improving performance  \n",
        "\n",
        "### ğŸš€ **Key Takeaways:**\n",
        "\n",
        "#### **1. Small Files Are Expensive:**\n",
        "- **Poor Query Performance**: More files = slower queries\n",
        "- **High Metadata Overhead**: Each file has overhead\n",
        "- **Storage Inefficiency**: Wasted space and resources\n",
        "- **High Costs**: More API calls and compute needed\n",
        "\n",
        "#### **2. Compaction Solves These Problems:**\n",
        "- **Fewer Files**: Reduced file count for faster scanning\n",
        "- **Larger Files**: Better I/O performance\n",
        "- **Storage Efficiency**: Reduced metadata overhead\n",
        "- **Cost Savings**: Lower storage and compute costs\n",
        "\n",
        "#### **3. Best Practices for Production:**\n",
        "\n",
        "##### **File Size Guidelines:**\n",
        "- **Target Size**: 128MB - 1GB per file\n",
        "- **Minimum Size**: Avoid files < 64KB\n",
        "- **Maximum Size**: Keep files < 2GB for optimal performance\n",
        "- **Compression**: Use appropriate compression (Snappy, Gzip, Zstd)\n",
        "\n",
        "##### **Compaction Strategies:**\n",
        "- **Rewrite Compaction**: Merge small files into larger ones\n",
        "- **Bin Packing**: Optimize file sizes for better performance\n",
        "- **Sort Compaction**: Sort data within files for optimal access\n",
        "- **Z-Order Compaction**: Optimize for multi-dimensional queries\n",
        "\n",
        "##### **Automation:**\n",
        "- **Scheduled Compaction**: Run compaction jobs regularly\n",
        "- **Threshold-based**: Trigger compaction when file count exceeds threshold\n",
        "- **Size-based**: Trigger when average file size drops below threshold\n",
        "- **Time-based**: Run compaction during low-traffic periods\n",
        "\n",
        "### ğŸ“š **Production Implementation:**\n",
        "\n",
        "#### **1. Apache Spark:**\n",
        "```python\n",
        "# Rewrite compaction with Spark\n",
        "spark.sql(\"\"\"\n",
        "    CALL system.rewrite_data_files(\n",
        "        table => 'catalog.database.table',\n",
        "        options => map('target-file-size-bytes', '134217728')\n",
        "    )\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "#### **2. Trino:**\n",
        "```sql\n",
        "-- Rewrite compaction with Trino\n",
        "CALL system.rewrite_data_files(\n",
        "    'catalog.database.table',\n",
        "    target_file_size => '128MB'\n",
        ");\n",
        "```\n",
        "\n",
        "#### **3. PyIceberg:**\n",
        "```python\n",
        "# Manual compaction with PyIceberg\n",
        "table.rewrite_data_files(\n",
        "    target_file_size_bytes=134217728  # 128MB\n",
        ")\n",
        "```\n",
        "\n",
        "### ğŸ¯ **Next Steps:**\n",
        "\n",
        "1. **Implement in Production**: Apply compaction to your data lakes\n",
        "2. **Monitor Performance**: Track query performance improvements\n",
        "3. **Automate Compaction**: Set up scheduled compaction jobs\n",
        "4. **Optimize Further**: Experiment with different compaction strategies\n",
        "5. **Scale Up**: Apply to larger datasets and more complex schemas\n",
        "\n",
        "### ğŸ“– **Additional Resources:**\n",
        "\n",
        "- [Apache Iceberg Compaction Documentation](https://iceberg.apache.org/docs/latest/maintenance/)\n",
        "- [Spark SQL Compaction](https://spark.apache.org/docs/latest/sql-data-sources-iceberg.html#compaction)\n",
        "- [Trino Iceberg Compaction](https://trino.io/docs/current/connector/iceberg.html#compaction)\n",
        "- [PyIceberg Compaction](https://py.iceberg.apache.org/operations/compaction/)\n",
        "\n",
        "**Happy compacting! ğŸš€**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
