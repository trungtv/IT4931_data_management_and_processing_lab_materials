{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Consumer Groups and Load Balancing\n",
        "\n",
        "## üéØ Objectives\n",
        "- Understand Consumer Groups and how they work\n",
        "- Learn about partition assignment strategies\n",
        "- Practice load balancing across multiple consumers\n",
        "- Explore consumer group coordination\n",
        "- Monitor consumer lag and performance\n",
        "\n",
        "## üìã Prerequisites\n",
        "- Lab 1 completed (Kafka basics)\n",
        "- Kafka cluster running\n",
        "- Understanding of partitions and topics\n",
        "\n",
        "## üèóÔ∏è Architecture Overview\n",
        "```\n",
        "Stock Data Topic (3 Partitions)\n",
        "         ‚Üì\n",
        "    Consumer Group A\n",
        "    ‚îú‚îÄ‚îÄ Consumer 1 (Partition 0)\n",
        "    ‚îú‚îÄ‚îÄ Consumer 2 (Partition 1)\n",
        "    ‚îî‚îÄ‚îÄ Consumer 3 (Partition 2)\n",
        "    \n",
        "    Consumer Group B\n",
        "    ‚îú‚îÄ‚îÄ Consumer 1 (All Partitions)\n",
        "    ‚îî‚îÄ‚îÄ Consumer 2 (Standby)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kafka-python in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.2.15)\n",
            "Requirement already satisfied: pandas in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: seaborn in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install and Import Dependencies\n",
        "%pip install kafka-python pandas matplotlib seaborn\n",
        "\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Configured for consumer groups:\n",
            "üîó Kafka Bootstrap Servers: localhost:9092\n",
            "üìù Topic Name: stock-data\n",
            "üë• Analytics Group: stock-analytics-group\n",
            "üë• Alerts Group: stock-alerts-group\n",
            "üë• Storage Group: stock-storage-group\n"
          ]
        }
      ],
      "source": [
        "# Kafka Configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
        "TOPIC_NAME = 'stock-data'\n",
        "\n",
        "# Consumer Group Names\n",
        "ANALYTICS_GROUP = 'stock-analytics-group'\n",
        "ALERTS_GROUP = 'stock-alerts-group'\n",
        "STORAGE_GROUP = 'stock-storage-group'\n",
        "\n",
        "print(f\"üìä Configured for consumer groups:\")\n",
        "print(f\"üîó Kafka Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
        "print(f\"üìù Topic Name: {TOPIC_NAME}\")\n",
        "print(f\"üë• Analytics Group: {ANALYTICS_GROUP}\")\n",
        "print(f\"üë• Alerts Group: {ALERTS_GROUP}\")\n",
        "print(f\"üë• Storage Group: {STORAGE_GROUP}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Stock data generator initialized!\n"
          ]
        }
      ],
      "source": [
        "# Stock Data Generator Class\n",
        "class StockDataGenerator:\n",
        "    def __init__(self, bootstrap_servers: str, topic: str):\n",
        "        self.producer = KafkaProducer(\n",
        "            bootstrap_servers=bootstrap_servers,\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            key_serializer=lambda k: k.encode('utf-8') if k else None\n",
        "        )\n",
        "        self.topic = topic\n",
        "        self.symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'META', 'NVDA', 'NFLX', 'ADBE', 'CRM']\n",
        "        self.base_prices = {\n",
        "            'AAPL': 150.0, 'GOOGL': 2800.0, 'MSFT': 350.0, 'TSLA': 250.0, 'AMZN': 3200.0,\n",
        "            'META': 300.0, 'NVDA': 450.0, 'NFLX': 400.0, 'ADBE': 500.0, 'CRM': 200.0\n",
        "        }\n",
        "    \n",
        "    def generate_ohlcv_data(self, symbol: str, base_price: float) -> dict:\n",
        "        \"\"\"Generate realistic OHLCV data\"\"\"\n",
        "        price_change = random.uniform(-0.02, 0.02)\n",
        "        new_price = base_price * (1 + price_change)\n",
        "        \n",
        "        open_price = round(new_price * random.uniform(0.998, 1.002), 2)\n",
        "        close_price = round(new_price * random.uniform(0.998, 1.002), 2)\n",
        "        high_price = round(max(open_price, close_price) * random.uniform(1.001, 1.005), 2)\n",
        "        low_price = round(min(open_price, close_price) * random.uniform(0.995, 0.999), 2)\n",
        "        \n",
        "        volume = random.randint(100000, 1000000)\n",
        "        \n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"timestamp\": datetime.now().isoformat() + \"Z\",\n",
        "            \"open\": open_price,\n",
        "            \"high\": high_price,\n",
        "            \"low\": low_price,\n",
        "            \"close\": close_price,\n",
        "            \"volume\": volume,\n",
        "            \"exchange\": \"NASDAQ\"\n",
        "        }\n",
        "    \n",
        "    def send_stock_data(self, num_messages: int = 10):\n",
        "        \"\"\"Send stock data to Kafka topic\"\"\"\n",
        "        print(f\"üìà Generating {num_messages} stock data messages...\")\n",
        "        \n",
        "        for i in range(num_messages):\n",
        "            symbol = random.choice(self.symbols)\n",
        "            base_price = self.base_prices[symbol]\n",
        "            ohlcv_data = self.generate_ohlcv_data(symbol, base_price)\n",
        "            \n",
        "            # Use symbol as key for partitioning\n",
        "            future = self.producer.send(self.topic, key=symbol, value=ohlcv_data)\n",
        "            record_metadata = future.get(timeout=10)\n",
        "            \n",
        "            print(f\"üìä Sent {symbol}: ${ohlcv_data['close']} -> Partition {record_metadata.partition}\")\n",
        "            time.sleep(0.1)  # Small delay between messages\n",
        "        \n",
        "        self.producer.flush()\n",
        "        print(f\"‚úÖ Successfully sent {num_messages} messages to topic '{self.topic}'\")\n",
        "\n",
        "# Initialize data generator\n",
        "data_generator = StockDataGenerator(KAFKA_BOOTSTRAP_SERVERS, TOPIC_NAME)\n",
        "print(\"‚úÖ Stock data generator initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Understanding Consumer Groups\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand how Consumer Groups work\n",
        "- Learn about partition assignment strategies\n",
        "- Observe load balancing in action\n",
        "- Monitor consumer group behavior\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Consumer Group**: A collection of consumers that work together to consume messages from topics\n",
        "2. **Partition Assignment**: Each partition is assigned to only one consumer in a group\n",
        "3. **Load Balancing**: Messages are distributed across consumers in the group\n",
        "4. **Group Coordination**: Kafka coordinates partition assignments automatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating multiple consumers in the same group...\n",
            "‚úÖ Created 2 consumers in analytics group\n",
            "üë• Group: stock-analytics-group\n",
            "üîß Consumer 1: analytics-consumer-1\n",
            "üîß Consumer 2: analytics-consumer-2\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Create Multiple Consumers in Same Group\n",
        "print(\"üîß Creating multiple consumers in the same group...\")\n",
        "\n",
        "def create_consumer(group_id: str, consumer_id: str):\n",
        "    \"\"\"Create a consumer with specific group and ID\"\"\"\n",
        "    return KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id=group_id,\n",
        "        client_id=consumer_id,\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        consumer_timeout_ms=5000  # Timeout after 5 seconds if no messages\n",
        "    )\n",
        "\n",
        "# Create consumers for analytics group\n",
        "analytics_consumer_1 = create_consumer(ANALYTICS_GROUP, 'analytics-consumer-1')\n",
        "analytics_consumer_2 = create_consumer(ANALYTICS_GROUP, 'analytics-consumer-2')\n",
        "\n",
        "print(\"‚úÖ Created 2 consumers in analytics group\")\n",
        "print(f\"üë• Group: {ANALYTICS_GROUP}\")\n",
        "print(f\"üîß Consumer 1: analytics-consumer-1\")\n",
        "print(f\"üîß Consumer 2: analytics-consumer-2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Generating test stock data...\n",
            "üìà Generating 20 stock data messages...\n",
            "üìä Sent TSLA: $252.81 -> Partition 0\n",
            "üìä Sent NVDA: $451.89 -> Partition 0\n",
            "üìä Sent ADBE: $506.71 -> Partition 0\n",
            "üìä Sent GOOGL: $2808.73 -> Partition 0\n",
            "üìä Sent NVDA: $442.19 -> Partition 0\n",
            "üìä Sent ADBE: $494.64 -> Partition 0\n",
            "üìä Sent NVDA: $453.14 -> Partition 0\n",
            "üìä Sent ADBE: $498.2 -> Partition 0\n",
            "üìä Sent AMZN: $3233.54 -> Partition 0\n",
            "üìä Sent AMZN: $3148.17 -> Partition 0\n",
            "üìä Sent CRM: $196.8 -> Partition 0\n",
            "üìä Sent NVDA: $453.65 -> Partition 0\n",
            "üìä Sent NFLX: $404.85 -> Partition 0\n",
            "üìä Sent NVDA: $442.24 -> Partition 0\n",
            "üìä Sent NFLX: $396.04 -> Partition 0\n",
            "üìä Sent MSFT: $351.94 -> Partition 0\n",
            "üìä Sent AMZN: $3226.96 -> Partition 0\n",
            "üìä Sent AAPL: $152.56 -> Partition 0\n",
            "üìä Sent AMZN: $3195.42 -> Partition 0\n",
            "üìä Sent MSFT: $343.58 -> Partition 0\n",
            "‚úÖ Successfully sent 20 messages to topic 'stock-data'\n",
            "‚úÖ Test data generated!\n",
            "üí° Now let's see how consumers handle the messages...\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: Generate Test Data\n",
        "print(\"üìà Generating test stock data...\")\n",
        "\n",
        "# Send some test data\n",
        "data_generator.send_stock_data(20)\n",
        "\n",
        "print(\"‚úÖ Test data generated!\")\n",
        "print(\"üí° Now let's see how consumers handle the messages...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Demonstrating consumer group load balancing...\n",
            "\n",
            "üîÑ Starting Consumer 1...\n",
            "üöÄ Analytics Consumer 1 starting to consume messages...\n",
            "üìä Analytics Consumer 1 received AAPL: $147.29 from partition 0\n",
            "üìä Analytics Consumer 1 received CRM: $200.24 from partition 0\n",
            "üìä Analytics Consumer 1 received MSFT: $353.31 from partition 0\n",
            "üìä Analytics Consumer 1 received AMZN: $3193.57 from partition 0\n",
            "üìä Analytics Consumer 1 received META: $300.17 from partition 0\n",
            "üìä Analytics Consumer 1 received TSLA: $249.58 from partition 0\n",
            "üìä Analytics Consumer 1 received NVDA: $453.01 from partition 0\n",
            "üìä Analytics Consumer 1 received NFLX: $401.11 from partition 0\n",
            "üìä Analytics Consumer 1 received TSLA: $248.7 from partition 0\n",
            "üìä Analytics Consumer 1 received NFLX: $401.48 from partition 0\n",
            "üìà Analytics Consumer 1 summary:\n",
            "   Total messages: 10\n",
            "   Partition distribution: {0: 10}\n",
            "\n",
            "üîÑ Starting Consumer 2...\n",
            "üöÄ Analytics Consumer 2 starting to consume messages...\n",
            "üìà Analytics Consumer 2 summary:\n",
            "   Total messages: 0\n",
            "   Partition distribution: {}\n",
            "\n",
            "üìä Load Balancing Results:\n",
            "Consumer 1 partitions: {0: 10}\n",
            "Consumer 2 partitions: {}\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: Consumer Group Load Balancing Demo\n",
        "print(\"üîÑ Demonstrating consumer group load balancing...\")\n",
        "\n",
        "def consume_messages(consumer, consumer_name: str, max_messages: int = 10):\n",
        "    \"\"\"Consume messages and track partition assignments\"\"\"\n",
        "    messages_received = 0\n",
        "    partition_counts = {}\n",
        "    \n",
        "    print(f\"üöÄ {consumer_name} starting to consume messages...\")\n",
        "    \n",
        "    try:\n",
        "        for message in consumer:\n",
        "            if messages_received >= max_messages:\n",
        "                break\n",
        "                \n",
        "            partition = message.partition\n",
        "            partition_counts[partition] = partition_counts.get(partition, 0) + 1\n",
        "            \n",
        "            data = message.value\n",
        "            print(f\"üìä {consumer_name} received {data['symbol']}: ${data['close']} from partition {partition}\")\n",
        "            \n",
        "            messages_received += 1\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è {consumer_name} finished consuming: {e}\")\n",
        "    \n",
        "    print(f\"üìà {consumer_name} summary:\")\n",
        "    print(f\"   Total messages: {messages_received}\")\n",
        "    print(f\"   Partition distribution: {partition_counts}\")\n",
        "    \n",
        "    return partition_counts\n",
        "\n",
        "# Start consuming with both consumers\n",
        "print(\"\\nüîÑ Starting Consumer 1...\")\n",
        "partition_counts_1 = consume_messages(analytics_consumer_1, \"Analytics Consumer 1\", 10)\n",
        "\n",
        "print(\"\\nüîÑ Starting Consumer 2...\")\n",
        "partition_counts_2 = consume_messages(analytics_consumer_2, \"Analytics Consumer 2\", 10)\n",
        "\n",
        "print(\"\\nüìä Load Balancing Results:\")\n",
        "print(f\"Consumer 1 partitions: {partition_counts_1}\")\n",
        "print(f\"Consumer 2 partitions: {partition_counts_2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Multiple Consumer Groups\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Understand how different consumer groups work independently\n",
        "- Learn about message replication across groups\n",
        "- Observe group coordination and rebalancing\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Independent Groups**: Each consumer group processes all messages independently\n",
        "2. **Message Replication**: Same message can be consumed by multiple groups\n",
        "3. **Group Isolation**: Groups don't interfere with each other's processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating consumers for different groups...\n",
            "‚úÖ Created consumers for different groups:\n",
            "üö® Alerts Group: stock-alerts-group\n",
            "üíæ Storage Group: stock-storage-group\n",
            "\n",
            "üìà Generating more test data...\n",
            "üìà Generating 15 stock data messages...\n",
            "üìä Sent AMZN: $3248.6 -> Partition 0\n",
            "üìä Sent AMZN: $3189.68 -> Partition 0\n",
            "üìä Sent TSLA: $248.63 -> Partition 0\n",
            "üìä Sent ADBE: $498.46 -> Partition 0\n",
            "üìä Sent GOOGL: $2749.07 -> Partition 0\n",
            "üìä Sent NVDA: $451.65 -> Partition 0\n",
            "üìä Sent NFLX: $407.11 -> Partition 0\n",
            "üìä Sent NVDA: $456.85 -> Partition 0\n",
            "üìä Sent AMZN: $3150.55 -> Partition 0\n",
            "üìä Sent ADBE: $503.86 -> Partition 0\n",
            "üìä Sent TSLA: $254.41 -> Partition 0\n",
            "üìä Sent NFLX: $407.73 -> Partition 0\n",
            "üìä Sent AAPL: $151.42 -> Partition 0\n",
            "üìä Sent CRM: $195.71 -> Partition 0\n",
            "üìä Sent NVDA: $455.48 -> Partition 0\n",
            "‚úÖ Successfully sent 15 messages to topic 'stock-data'\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: Create Multiple Consumer Groups\n",
        "print(\"üîß Creating consumers for different groups...\")\n",
        "\n",
        "# Create consumers for different groups\n",
        "alerts_consumer = create_consumer(ALERTS_GROUP, 'alerts-consumer')\n",
        "storage_consumer = create_consumer(STORAGE_GROUP, 'storage-consumer')\n",
        "\n",
        "print(\"‚úÖ Created consumers for different groups:\")\n",
        "print(f\"üö® Alerts Group: {ALERTS_GROUP}\")\n",
        "print(f\"üíæ Storage Group: {STORAGE_GROUP}\")\n",
        "\n",
        "# Generate more test data\n",
        "print(\"\\nüìà Generating more test data...\")\n",
        "data_generator.send_stock_data(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Demonstrating independent group processing...\n",
            "\n",
            "üö® Alerts Group Processing:\n",
            "   üìä AAPL: $147.29 - No alert\n",
            "   üö® HIGH PRICE ALERT: CRM at $200.24\n",
            "   üö® HIGH PRICE ALERT: MSFT at $353.31\n",
            "   üö® HIGH PRICE ALERT: AMZN at $3193.57\n",
            "   üö® HIGH PRICE ALERT: META at $300.17\n",
            "\n",
            "üíæ Storage Group Processing:\n",
            "   üíæ STORED: AAPL at 2025-09-21T15:56:47.006979Z\n",
            "   üíæ STORED: CRM at 2025-09-21T16:04:43.742567Z\n",
            "   üíæ STORED: MSFT at 2025-09-21T16:04:43.977061Z\n",
            "   üíæ STORED: AMZN at 2025-09-21T16:04:44.094246Z\n",
            "   üíæ STORED: META at 2025-09-21T16:04:44.215153Z\n",
            "\n",
            "‚úÖ Both groups processed messages independently!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 5: Demonstrate Independent Group Processing\n",
        "print(\"üîÑ Demonstrating independent group processing...\")\n",
        "\n",
        "def process_alerts(message_data):\n",
        "    \"\"\"Process stock data for alerts\"\"\"\n",
        "    symbol = message_data['symbol']\n",
        "    close_price = message_data['close']\n",
        "    \n",
        "    # Simple alert logic: alert if price change is significant\n",
        "    if close_price > 200:  # High price alert\n",
        "        return f\"üö® HIGH PRICE ALERT: {symbol} at ${close_price}\"\n",
        "    return None\n",
        "\n",
        "def process_storage(message_data):\n",
        "    \"\"\"Process stock data for storage\"\"\"\n",
        "    symbol = message_data['symbol']\n",
        "    timestamp = message_data['timestamp']\n",
        "    \n",
        "    # Simulate storing to database\n",
        "    return f\"üíæ STORED: {symbol} at {timestamp}\"\n",
        "\n",
        "# Process messages with different groups\n",
        "print(\"\\nüö® Alerts Group Processing:\")\n",
        "alerts_processed = 0\n",
        "for message in alerts_consumer:\n",
        "    if alerts_processed >= 5:\n",
        "        break\n",
        "    \n",
        "    data = message.value\n",
        "    alert = process_alerts(data)\n",
        "    if alert:\n",
        "        print(f\"   {alert}\")\n",
        "    else:\n",
        "        print(f\"   üìä {data['symbol']}: ${data['close']} - No alert\")\n",
        "    \n",
        "    alerts_processed += 1\n",
        "\n",
        "print(\"\\nüíæ Storage Group Processing:\")\n",
        "storage_processed = 0\n",
        "for message in storage_consumer:\n",
        "    if storage_processed >= 5:\n",
        "        break\n",
        "    \n",
        "    data = message.value\n",
        "    storage_result = process_storage(data)\n",
        "    print(f\"   {storage_result}\")\n",
        "    \n",
        "    storage_processed += 1\n",
        "\n",
        "print(\"\\n‚úÖ Both groups processed messages independently!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Consumer Group Monitoring\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Learn how to monitor consumer groups\n",
        "- Understand consumer lag and performance metrics\n",
        "- Practice troubleshooting consumer issues\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Consumer Lag**: Difference between producer and consumer offsets\n",
        "2. **Group Coordination**: How Kafka manages group membership\n",
        "3. **Rebalancing**: Automatic redistribution of partitions when consumers join/leave\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Monitoring consumer groups...\n",
            "üîç Monitoring consumer groups...\n",
            "üìã Available Consumer Groups:\n",
            "   üë• Group: stock-alerts-group, Type: consumer\n",
            "   üë• Group: stock-analytics-group, Type: consumer\n",
            "   üë• Group: stock-storage-group, Type: consumer\n",
            "\n",
            "üìä Monitoring group: stock-analytics-group\n",
            "\n",
            "üìä Partition info for topic 'stock-data':\n",
            "   Partitions: {0}\n",
            "\n",
            "üìà Committed offsets for group 'stock-analytics-group':\n",
            "   No committed offsets found for group 'stock-analytics-group'\n",
            "\n",
            "üìä Monitoring group: stock-alerts-group\n",
            "\n",
            "üìä Partition info for topic 'stock-data':\n",
            "   Partitions: {0}\n",
            "\n",
            "üìà Committed offsets for group 'stock-alerts-group':\n",
            "   No committed offsets found for group 'stock-alerts-group'\n",
            "\n",
            "üìä Monitoring group: stock-storage-group\n",
            "\n",
            "üìä Partition info for topic 'stock-data':\n",
            "   Partitions: {0}\n",
            "\n",
            "üìà Committed offsets for group 'stock-storage-group':\n",
            "   No committed offsets found for group 'stock-storage-group'\n",
            "\n",
            "‚úÖ Consumer group monitoring completed!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 6: Monitor Consumer Groups\n",
        "print(\"üìä Monitoring consumer groups...\")\n",
        "\n",
        "from kafka.admin import KafkaAdminClient, ConfigResource, ConfigResourceType\n",
        "\n",
        "def get_consumer_group_info():\n",
        "    \"\"\"Get information about consumer groups\"\"\"\n",
        "    try:\n",
        "        admin_client = KafkaAdminClient(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            client_id='monitor-client'\n",
        "        )\n",
        "        \n",
        "        # List all consumer groups\n",
        "        groups = admin_client.list_consumer_groups()\n",
        "        print(\"üìã Available Consumer Groups:\")\n",
        "        for group in groups:\n",
        "            print(f\"   üë• Group: {group[0]}, Type: {group[1]}\")\n",
        "        \n",
        "        return groups\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting consumer group info: {e}\")\n",
        "        return []\n",
        "\n",
        "def monitor_group_offsets(group_id: str):\n",
        "    \"\"\"Monitor offsets for a specific consumer group\"\"\"\n",
        "    try:\n",
        "        consumer = KafkaConsumer(\n",
        "            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "            group_id=group_id,\n",
        "            enable_auto_commit=False\n",
        "        )\n",
        "        \n",
        "        # Get partition info for the topic\n",
        "        partitions = consumer.partitions_for_topic(TOPIC_NAME)\n",
        "        print(f\"\\nüìä Partition info for topic '{TOPIC_NAME}':\")\n",
        "        print(f\"   Partitions: {partitions}\")\n",
        "        \n",
        "        # Get committed offsets\n",
        "        if partitions:\n",
        "            from kafka import TopicPartition\n",
        "            topic_partitions = [TopicPartition(TOPIC_NAME, p) for p in partitions]\n",
        "            committed_offsets = consumer.committed(*topic_partitions)\n",
        "            \n",
        "            print(f\"\\nüìà Committed offsets for group '{group_id}':\")\n",
        "            if committed_offsets is not None:\n",
        "                for tp, offset in zip(topic_partitions, committed_offsets):\n",
        "                    if offset is not None:\n",
        "                        print(f\"   Partition {tp.partition}: {offset}\")\n",
        "                    else:\n",
        "                        print(f\"   Partition {tp.partition}: No committed offset\")\n",
        "            else:\n",
        "                print(f\"   No committed offsets found for group '{group_id}'\")\n",
        "        \n",
        "        consumer.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error monitoring offsets: {e}\")\n",
        "\n",
        "# Monitor all our consumer groups\n",
        "print(\"üîç Monitoring consumer groups...\")\n",
        "groups = get_consumer_group_info()\n",
        "\n",
        "for group_id in [ANALYTICS_GROUP, ALERTS_GROUP, STORAGE_GROUP]:\n",
        "    print(f\"\\nüìä Monitoring group: {group_id}\")\n",
        "    monitor_group_offsets(group_id)\n",
        "\n",
        "print(\"\\n‚úÖ Consumer group monitoring completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Advanced Consumer Group Scenarios\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Practice consumer group rebalancing\n",
        "- Understand partition assignment strategies\n",
        "- Learn about consumer group coordination\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Rebalancing**: Automatic redistribution when consumers join/leave\n",
        "2. **Assignment Strategies**: Range, Round Robin, Sticky\n",
        "3. **Group Coordination**: How Kafka manages group membership\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Demonstrating consumer group rebalancing...\n",
            "üîß Creating consumers for rebalancing demo group: rebalance-demo-group\n",
            "\n",
            "üìä Step 1: Starting with 1 consumer...\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'StickyPartitionAssignor' from 'kafka.coordinator.assignors.sticky' (/Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages/kafka/coordinator/assignors/sticky/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Start with 1 consumer\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Step 1: Starting with 1 consumer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m consumer_1 \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_consumer_with_assignment_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREBALANCE_GROUP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrebalance-consumer-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Generate some data\u001b[39;00m\n\u001b[1;32m     42\u001b[0m data_generator\u001b[38;5;241m.\u001b[39msend_stock_data(\u001b[38;5;241m10\u001b[39m)\n",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mcreate_consumer_with_assignment_strategy\u001b[0;34m(group_id, consumer_id, strategy)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoordinator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massignors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrange\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RangePartitionAssignor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoordinator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massignors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroundrobin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RoundRobinPartitionAssignor\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoordinator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massignors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msticky\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StickyPartitionAssignor\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Map strategy names to actual assignor classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m assignor_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m'\u001b[39m: RangePartitionAssignor,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroundrobin\u001b[39m\u001b[38;5;124m'\u001b[39m: RoundRobinPartitionAssignor,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msticky\u001b[39m\u001b[38;5;124m'\u001b[39m: StickyPartitionAssignor\n\u001b[1;32m     15\u001b[0m }\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'StickyPartitionAssignor' from 'kafka.coordinator.assignors.sticky' (/Users/trungtv/miniforge3/envs/datalab/lib/python3.10/site-packages/kafka/coordinator/assignors/sticky/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Exercise 7: Consumer Group Rebalancing Demo\n",
        "print(\"üîÑ Demonstrating consumer group rebalancing...\")\n",
        "\n",
        "def create_consumer_with_assignment_strategy(group_id: str, consumer_id: str, strategy: str = 'range'):\n",
        "    \"\"\"Create consumer with specific assignment strategy\"\"\"\n",
        "    from kafka.coordinator.assignors.range import RangePartitionAssignor\n",
        "    from kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor\n",
        "    from kafka.coordinator.assignors.sticky import StickyPartitionAssignor\n",
        "    \n",
        "    # Map strategy names to actual assignor classes\n",
        "    assignor_map = {\n",
        "        'range': RangePartitionAssignor,\n",
        "        'roundrobin': RoundRobinPartitionAssignor,\n",
        "        'sticky': StickyPartitionAssignor\n",
        "    }\n",
        "    \n",
        "    assignor_class = assignor_map.get(strategy, RangePartitionAssignor)\n",
        "    \n",
        "    return KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id=group_id,\n",
        "        client_id=consumer_id,\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        consumer_timeout_ms=3000,\n",
        "        # Assignment strategy - use class objects, not strings\n",
        "        partition_assignment_strategy=[assignor_class()]\n",
        "    )\n",
        "\n",
        "# Create a new group for rebalancing demo\n",
        "REBALANCE_GROUP = 'rebalance-demo-group'\n",
        "\n",
        "print(f\"üîß Creating consumers for rebalancing demo group: {REBALANCE_GROUP}\")\n",
        "\n",
        "# Start with 1 consumer\n",
        "print(\"\\nüìä Step 1: Starting with 1 consumer...\")\n",
        "consumer_1 = create_consumer_with_assignment_strategy(REBALANCE_GROUP, 'rebalance-consumer-1')\n",
        "\n",
        "# Generate some data\n",
        "data_generator.send_stock_data(10)\n",
        "\n",
        "# Consume with 1 consumer\n",
        "print(\"\\nüîÑ Consumer 1 consuming messages...\")\n",
        "messages_1 = []\n",
        "for message in consumer_1:\n",
        "    if len(messages_1) >= 5:\n",
        "        break\n",
        "    messages_1.append(message)\n",
        "    print(f\"   Consumer 1: {message.value['symbol']} from partition {message.partition}\")\n",
        "\n",
        "print(f\"\\nüìà Consumer 1 processed {len(messages_1)} messages\")\n",
        "\n",
        "# Add second consumer (this will trigger rebalancing)\n",
        "print(\"\\nüìä Step 2: Adding second consumer (rebalancing)...\")\n",
        "consumer_2 = create_consumer_with_assignment_strategy(REBALANCE_GROUP, 'rebalance-consumer-2')\n",
        "\n",
        "# Generate more data\n",
        "data_generator.send_stock_data(10)\n",
        "\n",
        "# Both consumers should now share the load\n",
        "print(\"\\nüîÑ Both consumers processing messages...\")\n",
        "messages_2 = []\n",
        "for message in consumer_2:\n",
        "    if len(messages_2) >= 5:\n",
        "        break\n",
        "    messages_2.append(message)\n",
        "    print(f\"   Consumer 2: {message.value['symbol']} from partition {message.partition}\")\n",
        "\n",
        "print(f\"\\nüìà Consumer 2 processed {len(messages_2)} messages\")\n",
        "print(\"\\n‚úÖ Rebalancing demonstration completed!\")\n",
        "\n",
        "# Clean up consumers\n",
        "consumer_1.close()\n",
        "consumer_2.close()\n",
        "print(\"üßπ Consumers closed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 8: Performance Analysis and Visualization\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Analyze consumer group performance\n",
        "- Visualize message distribution across partitions\n",
        "- Understand throughput and latency patterns\n",
        "\n",
        "### üìö **Key Concepts:**\n",
        "1. **Throughput**: Messages processed per second\n",
        "2. **Latency**: Time between message production and consumption\n",
        "3. **Partition Distribution**: How messages are distributed across partitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 8: Performance Analysis and Visualization\n",
        "print(\"üìä Analyzing consumer group performance...\")\n",
        "\n",
        "def analyze_performance():\n",
        "    \"\"\"Analyze performance of consumer groups\"\"\"\n",
        "    \n",
        "    # Create performance tracking consumer\n",
        "    perf_consumer = KafkaConsumer(\n",
        "        TOPIC_NAME,\n",
        "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
        "        group_id='performance-analysis-group',\n",
        "        auto_offset_reset='earliest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "        consumer_timeout_ms=5000\n",
        "    )\n",
        "    \n",
        "    # Generate test data\n",
        "    print(\"üìà Generating performance test data...\")\n",
        "    data_generator.send_stock_data(30)\n",
        "    \n",
        "    # Track performance metrics\n",
        "    partition_counts = defaultdict(int)\n",
        "    symbol_counts = defaultdict(int)\n",
        "    processing_times = []\n",
        "    \n",
        "    print(\"\\nüîÑ Analyzing message processing...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for message in perf_consumer:\n",
        "        process_start = time.time()\n",
        "        \n",
        "        # Track partition distribution\n",
        "        partition_counts[message.partition] += 1\n",
        "        \n",
        "        # Track symbol distribution\n",
        "        symbol_counts[message.value['symbol']] += 1\n",
        "        \n",
        "        # Simulate processing time\n",
        "        time.sleep(0.01)  # 10ms processing time\n",
        "        \n",
        "        process_end = time.time()\n",
        "        processing_times.append(process_end - process_start)\n",
        "        \n",
        "        if len(processing_times) >= 20:  # Analyze first 20 messages\n",
        "            break\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    throughput = len(processing_times) / total_time if total_time > 0 else 0\n",
        "    avg_latency = sum(processing_times) / len(processing_times) if processing_times else 0\n",
        "    \n",
        "    print(f\"\\nüìä Performance Analysis Results:\")\n",
        "    print(f\"   Total messages processed: {len(processing_times)}\")\n",
        "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"   Throughput: {throughput:.2f} messages/second\")\n",
        "    print(f\"   Average processing latency: {avg_latency*1000:.2f} ms\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Consumer Group Performance Analysis', fontsize=16)\n",
        "    \n",
        "    # Partition distribution\n",
        "    axes[0, 0].bar(partition_counts.keys(), partition_counts.values())\n",
        "    axes[0, 0].set_title('Message Distribution by Partition')\n",
        "    axes[0, 0].set_xlabel('Partition')\n",
        "    axes[0, 0].set_ylabel('Message Count')\n",
        "    \n",
        "    # Symbol distribution\n",
        "    axes[0, 1].bar(symbol_counts.keys(), symbol_counts.values())\n",
        "    axes[0, 1].set_title('Message Distribution by Stock Symbol')\n",
        "    axes[0, 1].set_xlabel('Stock Symbol')\n",
        "    axes[0, 1].set_ylabel('Message Count')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Processing time distribution\n",
        "    axes[1, 0].hist(processing_times, bins=10, alpha=0.7)\n",
        "    axes[1, 0].set_title('Processing Time Distribution')\n",
        "    axes[1, 0].set_xlabel('Processing Time (seconds)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # Throughput over time\n",
        "    throughput_over_time = [1/t for t in processing_times if t > 0]\n",
        "    axes[1, 1].plot(throughput_over_time)\n",
        "    axes[1, 1].set_title('Throughput Over Time')\n",
        "    axes[1, 1].set_xlabel('Message Number')\n",
        "    axes[1, 1].set_ylabel('Messages/Second')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    perf_consumer.close()\n",
        "    \n",
        "    return {\n",
        "        'partition_counts': dict(partition_counts),\n",
        "        'symbol_counts': dict(symbol_counts),\n",
        "        'throughput': throughput,\n",
        "        'avg_latency': avg_latency\n",
        "    }\n",
        "\n",
        "# Run performance analysis\n",
        "performance_results = analyze_performance()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 9: Cleanup and Best Practices\n",
        "\n",
        "### üéØ **Learning Objectives:**\n",
        "- Learn proper cleanup procedures\n",
        "- Understand best practices for consumer groups\n",
        "- Review key takeaways from the lab\n",
        "\n",
        "### üìö **Best Practices:**\n",
        "1. **Always close consumers** to free resources\n",
        "2. **Use appropriate group IDs** for different use cases\n",
        "3. **Monitor consumer lag** in production\n",
        "4. **Handle rebalancing** gracefully in applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 9: Cleanup and Best Practices\n",
        "print(\"üßπ Cleaning up resources...\")\n",
        "\n",
        "def cleanup_consumers():\n",
        "    \"\"\"Properly close all consumers\"\"\"\n",
        "    consumers_to_close = [\n",
        "        analytics_consumer_1,\n",
        "        analytics_consumer_2,\n",
        "        alerts_consumer,\n",
        "        storage_consumer\n",
        "    ]\n",
        "    \n",
        "    for consumer in consumers_to_close:\n",
        "        try:\n",
        "            consumer.close()\n",
        "            print(f\"‚úÖ Closed consumer: {consumer.config['client_id']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error closing consumer: {e}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ All consumers closed successfully!\")\n",
        "\n",
        "def cleanup_producer():\n",
        "    \"\"\"Properly close producer\"\"\"\n",
        "    try:\n",
        "        data_generator.producer.close()\n",
        "        print(\"‚úÖ Producer closed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error closing producer: {e}\")\n",
        "\n",
        "# Cleanup resources\n",
        "cleanup_consumers()\n",
        "cleanup_producer()\n",
        "\n",
        "print(\"\\nüìö Lab Summary:\")\n",
        "print(\"‚úÖ Learned about Consumer Groups and Load Balancing\")\n",
        "print(\"‚úÖ Practiced Multiple Consumer Groups\")\n",
        "print(\"‚úÖ Demonstrated Consumer Group Rebalancing\")\n",
        "print(\"‚úÖ Analyzed Performance and Created Visualizations\")\n",
        "print(\"‚úÖ Applied Best Practices for Resource Management\")\n",
        "\n",
        "print(\"\\nüéØ Key Takeaways:\")\n",
        "print(\"1. Consumer Groups enable horizontal scaling of message processing\")\n",
        "print(\"2. Each partition is consumed by only one consumer in a group\")\n",
        "print(\"3. Multiple groups can process the same messages independently\")\n",
        "print(\"4. Kafka automatically handles partition assignment and rebalancing\")\n",
        "print(\"5. Monitoring consumer lag is crucial for production systems\")\n",
        "\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"- Try Lab 3: Partitioning Strategies\")\n",
        "print(\"- Experiment with different assignment strategies\")\n",
        "print(\"- Practice with real-world data scenarios\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datalab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
